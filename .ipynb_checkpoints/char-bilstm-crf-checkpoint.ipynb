{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ec6306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Field,NestedField, BucketIterator\n",
    "import underthesea\n",
    "from underthesea import word_tokenize\n",
    "from utils import read_file\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "from torchcrf import CRF\n",
    "class Dataset:\n",
    "    def __init__(self, train_path, val_path, test_path, batch_size):\n",
    "        self.word_field = Field(lower=True)\n",
    "        self.tag_field = Field(unk_token=None)\n",
    "        self.char_nesting_field = Field(tokenize=list)\n",
    "        self.char_field = NestedField(self.char_nesting_field)  # [batch_size, sent len, word len]\n",
    "        \n",
    "        self.datafields = [((\"word\", \"char\"), (self.word_field, self.char_field)),\n",
    "                (\"tag\", self.tag_field)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.train_dataset = read_file(train_path, self.datafields)\n",
    "        self.val_dataset = read_file(val_path, self.datafields)\n",
    "        self.test_dataset = read_file(test_path, self.datafields)\n",
    "\n",
    "        self.word_field.build_vocab(self.train_dataset.word)\n",
    "        self.char_field.build_vocab(self.train_dataset.char)  # NEWLY ADDED\n",
    "        self.tag_field.build_vocab(self.train_dataset.tag)\n",
    "\n",
    "        self.train_iter, self.val_iter, self.test_iter = BucketIterator.splits(\n",
    "            datasets=(self.train_dataset, self.val_dataset, self.test_dataset),\n",
    "            batch_size=batch_size, sort=False)\n",
    "\n",
    "        self.char_pad_idx = self.char_field.vocab.stoi[self.char_field.pad_token]  # NEWLY ADDED\n",
    "        self.word_pad_idx = self.word_field.vocab.stoi[self.word_field.pad_token]\n",
    "        self.tag_pad_idx = self.tag_field.vocab.stoi[self.tag_field.pad_token]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ae4176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 5027 sentences\n",
      "Val set: 2000 sentences\n",
      "Test set: 3000 sentences\n"
     ]
    }
   ],
   "source": [
    "corpus = Dataset(train_path='./PhoNER_COVID19/data/word/train_word.conll',\n",
    "                    val_path='./PhoNER_COVID19/data/word/dev_word.conll',\n",
    "                    test_path='./PhoNER_COVID19/data/word/test_word.conll',\n",
    "                    batch_size=64)\n",
    "\n",
    "print(f\"Train set: {len(corpus.train_dataset)} sentences\")\n",
    "print(f\"Val set: {len(corpus.val_dataset)} sentences\")\n",
    "print(f\"Test set: {len(corpus.test_dataset)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "175da58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['đồng_thời', ',', 'bệnh_viện', 'tiếp_tục', 'thực_hiện', 'các', 'biện_pháp', 'phòng_chống', 'dịch_bệnh', 'covid', '-', '00', 'theo', 'hướng_dẫn', 'của', 'bộ', 'y_tế', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'O']\n",
      "[['Đ', 'ồ', 'n', 'g', '_', 't', 'h', 'ờ', 'i'], [','], ['b', 'ệ', 'n', 'h', '_', 'v', 'i', 'ệ', 'n'], ['t', 'i', 'ế', 'p', '_', 't', 'ụ', 'c'], ['t', 'h', 'ự', 'c', '_', 'h', 'i', 'ệ', 'n'], ['c', 'á', 'c'], ['b', 'i', 'ệ', 'n', '_', 'p', 'h', 'á', 'p'], ['p', 'h', 'ò', 'n', 'g', '_', 'c', 'h', 'ố', 'n', 'g'], ['d', 'ị', 'c', 'h', '_', 'b', 'ệ', 'n', 'h'], ['C', 'O', 'V', 'I', 'D'], ['-'], ['0', '0'], ['t', 'h', 'e', 'o'], ['h', 'ư', 'ớ', 'n', 'g', '_', 'd', 'ẫ', 'n'], ['c', 'ủ', 'a'], ['B', 'ộ'], ['Y', '_', 't', 'ế'], ['.']]\n"
     ]
    }
   ],
   "source": [
    "for x in corpus.train_dataset:\n",
    "    print(x.word)\n",
    "    print(x.tag)\n",
    "    print(x.char)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f732f048",
   "metadata": {},
   "outputs": [],
   "source": [
    " def normalize_word(word):\n",
    "    new_word = \"\"\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            new_word += '0'\n",
    "        else:\n",
    "            new_word += char.lower()\n",
    "    return new_word\n",
    "\n",
    "def read_file(path):\n",
    "    with open(path, encoding = 'utf-8') as f:\n",
    "        examples = []\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                examples.append([words, tags])\n",
    "                words = []\n",
    "                tags = []\n",
    "            else:\n",
    "                columns = line.split()\n",
    "                words.append(normalize_word(columns[0]))\n",
    "                tags.append(columns[-1])\n",
    "    return examples\n",
    "\n",
    "data = read_file('./PhoNER_COVID19/data/word/train_word.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d40ff05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bà', 'này', 'khi', 'trở', 'về', 'quá_cảnh', 'doha', '(', 'qatar', ')', ',', 'đáp', 'xuống', 'tân_sơn_nhất', 'sáng', '0/0', 'cùng', '00', 'hành_khách', ',', 'trong', 'đó', 'có', '00', 'người', 'nước_ngoài', '.']\n"
     ]
    }
   ],
   "source": [
    "print(data[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42f8ca91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([77, 64])\n",
      "The shape of the character input follows [batch size, max number of words in the batch, max number of characters in a word within the batch]:\n",
      "torch.Size([64, 77, 17])\n"
     ]
    }
   ],
   "source": [
    "sample_train = next(iter(corpus.train_iter))\n",
    "print(sample_train.word.shape)\n",
    "print(\"The shape of the character input follows [batch size, max number of words in the batch, max number of characters in a word within the batch]:\")\n",
    "print(sample_train.char.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebc15fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character input for a sample sentence in the first train batch:\n",
      "S a u\n",
      "đ ó\n",
      ",\n",
      "m ẫ u\n",
      "b ệ n h _ p h ẩ m\n",
      "n à y\n",
      "đ ư ợ c\n",
      "g ử i\n",
      "n g a y\n",
      "đ ế n\n",
      "C D C\n",
      "H à _ N ộ i\n",
      "đ ể\n",
      "l à m\n",
      "x é t _ n g h i ệ m\n",
      "k h ẳ n g _ đ ị n h\n",
      "v à\n",
      "c ó\n",
      "k ế t _ q u ả\n",
      "d ư ơ n g _ t í n h\n",
      ".\n",
      "\n",
      "As you can see, we can preserve the capital cases as useful information with character-based representation.\n"
     ]
    }
   ],
   "source": [
    "sample_char_field = sample_train.char[0, :, :]\n",
    "char_pad_id = corpus.char_pad_idx\n",
    "print(\"Character input for a sample sentence in the first train batch:\")\n",
    "for word in sample_char_field:\n",
    "  if word[0] != char_pad_id:\n",
    "    print(\" \".join([corpus.char_field.vocab.itos[char] for char in word if char != char_pad_id]))\n",
    "print()\n",
    "print(\"As you can see, we can preserve the capital cases as useful information with character-based representation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a251328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 embedding_dim,\n",
    "                 char_emb_dim,\n",
    "                 char_input_dim,\n",
    "                 char_cnn_filter_num,\n",
    "                 char_cnn_kernel_size,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 lstm_layers,\n",
    "                 emb_dropout,\n",
    "                 cnn_dropout,\n",
    "                 lstm_dropout,\n",
    "                 fc_dropout,\n",
    "                 word_pad_idx,\n",
    "                 char_pad_idx,\n",
    "                 tag_pad_idx):\n",
    "        super().__init__()\n",
    "        # LAYER 1A: Word Embedding\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=input_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=word_pad_idx\n",
    "        )\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "        # LAYER 1B: Char Embedding-CNN\n",
    "        self.char_emb_dim = char_emb_dim\n",
    "        self.char_emb = nn.Embedding(\n",
    "            num_embeddings=char_input_dim,\n",
    "            embedding_dim=char_emb_dim,\n",
    "            padding_idx=char_pad_idx\n",
    "        )\n",
    "        self.char_cnn = nn.Conv1d(\n",
    "            in_channels=char_emb_dim,\n",
    "            out_channels=char_emb_dim * char_cnn_filter_num,\n",
    "            kernel_size=char_cnn_kernel_size,\n",
    "            groups=char_emb_dim  # different 1d conv for each embedding dim\n",
    "        )\n",
    "        self.cnn_dropout = nn.Dropout(cnn_dropout)\n",
    "        # LAYER 2: BiLSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim + (char_emb_dim * char_cnn_filter_num),\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=lstm_dropout if lstm_layers > 1 else 0\n",
    "        )\n",
    "        # LAYER 3: Fully-connected\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # times 2 for bidirectional\n",
    "        ### BEGIN MODIFIED SECTION: CRF OUTPUT ###\n",
    "        # LAYER 4: CRF\n",
    "        self.tag_pad_idx = tag_pad_idx\n",
    "        self.crf = CRF(num_tags=output_dim)\n",
    "        ### END MODIFIED SECTION ###\n",
    "        # init weights from normal distribution\n",
    "        for name, param in self.named_parameters():\n",
    "            nn.init.normal_(param.data, mean=0, std=0.1)\n",
    "\n",
    "    def forward(self, words, chars, tags=None):\n",
    "        # words = [sentence length, batch size]\n",
    "        # chars = [batch size, sentence length, word length)\n",
    "        # tags = [sentence length, batch size]\n",
    "        # embedding_out = [sentence length, batch size, embedding dim]\n",
    "        embedding_out = self.emb_dropout(self.embedding(words))\n",
    "        # character cnn layer forward\n",
    "        # reference: https://github.com/achernodub/targer/blob/master/src/layers/layer_char_cnn.py\n",
    "        # char_emb_out = [batch size, sentence length, word length, char emb dim]\n",
    "        char_emb_out = self.emb_dropout(self.char_emb(chars))\n",
    "        batch_size, sent_len, word_len, char_emb_dim = char_emb_out.shape\n",
    "        char_cnn_max_out = torch.zeros(batch_size, sent_len, self.char_cnn.out_channels)\n",
    "        for sent_i in range(sent_len):\n",
    "            # sent_char_emb = [batch size, word length, char emb dim]\n",
    "            sent_char_emb = char_emb_out[:, sent_i, :, :]\n",
    "            # sent_char_emb_p = [batch size, char emb dim, word length]\n",
    "            sent_char_emb_p = sent_char_emb.permute(0, 2, 1)\n",
    "            # char_cnn_sent_out = [batch size, out channels * char emb dim, word length - kernel size + 1]\n",
    "            char_cnn_sent_out = self.char_cnn(sent_char_emb_p)\n",
    "            char_cnn_max_out[:, sent_i, :], _ = torch.max(char_cnn_sent_out, dim=2)\n",
    "        char_cnn = self.cnn_dropout(char_cnn_max_out)\n",
    "        # concat word and char embedding\n",
    "        # char_cnn_p = [sentence length, batch size, char emb dim * num filter]\n",
    "        char_cnn_p = char_cnn.permute(1, 0, 2)\n",
    "        word_features = torch.cat((embedding_out, char_cnn_p), dim=2)\n",
    "        # lstm_out = [sentence length, batch size, hidden dim * 2]\n",
    "        lstm_out, _ = self.lstm(word_features)\n",
    "        # ner_out = [sentence length, batch size, output dim]\n",
    "        fc_out = self.fc(self.fc_dropout(lstm_out))\n",
    "        ### BEGIN MODIFIED SECTION: CRF ###\n",
    "        if tags is not None:\n",
    "            mask = tags != self.tag_pad_idx\n",
    "            crf_out = self.crf.decode(fc_out, mask=mask)\n",
    "            crf_loss = -self.crf(fc_out, tags=tags, mask=mask)\n",
    "        else:\n",
    "            crf_out = self.crf.decode(fc_out)\n",
    "            crf_loss = None\n",
    "        ### END MODIFIED SECTION ###\n",
    "        return crf_out, crf_loss\n",
    "\n",
    "    def init_embeddings(self, char_pad_idx, word_pad_idx):\n",
    "        # initialize embedding for padding as zero\n",
    "        self.embedding.weight.data[word_pad_idx] = torch.zeros(self.embedding_dim)\n",
    "        self.char_emb.weight.data[char_pad_idx] = torch.zeros(self.char_emb_dim)\n",
    "\n",
    "    ### BEGIN MODIFIED SECTION: CRF OUTPUT ###\n",
    "    def init_crf_transitions(self, tag_names, imp_value=-1e4):\n",
    "        num_tags = len(tag_names)\n",
    "        for i in range(num_tags):\n",
    "            tag_name = tag_names[i]\n",
    "            # I and L and <pad> impossible as a start\n",
    "            if tag_name[0] in (\"I\") or tag_name == \"<pad>\":\n",
    "                torch.nn.init.constant_(self.crf.start_transitions[i], imp_value)\n",
    "            # B and I impossible as an end\n",
    "            if tag_name[0] in (\"B\"):\n",
    "                torch.nn.init.constant_(self.crf.end_transitions[i], imp_value)\n",
    "        \n",
    "        # init impossible transitions between positions\n",
    "        tag_is = {}\n",
    "        for tag_position in (\"B\", \"I\", \"O\"):\n",
    "            tag_is[tag_position] = [i for i, tag in enumerate(tag_names) if tag[0] == tag_position]\n",
    "        tag_is[\"P\"] = [i for i, tag in enumerate(tag_names) if tag == \"tag\"]\n",
    "        \n",
    "        print(\"tag_is: \")\n",
    "        print(tag_is)\n",
    "        \n",
    "#         impossible_transitions_position = {\n",
    "#             \"B\": \"BOUP\",\n",
    "#             \"I\": \"BOUP\",\n",
    "#             \"O\": \"IL\",\n",
    "#             \"U\": \"IL\",\n",
    "#             \"L\": \"IL\"\n",
    "#         }\n",
    "#         for from_tag, to_tag_list in impossible_transitions_position.items():\n",
    "#             to_tags = list(to_tag_list)\n",
    "#             for from_tag_i in tag_is[from_tag]:\n",
    "#                 for to_tag in to_tags:\n",
    "#                     for to_tag_i in tag_is[to_tag]:\n",
    "#                         torch.nn.init.constant_(\n",
    "#                             self.crf.transitions[from_tag_i, to_tag_i], imp_value\n",
    "#                         )\n",
    "#         # init impossible B and I transitions to different entity types\n",
    "#         impossible_transitions_tags = {\n",
    "#             \"B\": \"IL\",\n",
    "#             \"I\": \"IL\"\n",
    "#         }\n",
    "#         for from_tag, to_tag_list in impossible_transitions_tags.items():\n",
    "#             to_tags = list(to_tag_list)\n",
    "#             for from_tag_i in tag_is[from_tag]:\n",
    "#                 for to_tag in to_tags:\n",
    "#                     for to_tag_i in tag_is[to_tag]:\n",
    "#                         if tag_names[from_tag_i].split(\"-\")[1] != tag_names[to_tag_i].split(\"-\")[1]:\n",
    "#                             torch.nn.init.constant_(\n",
    "#                                 self.crf.transitions[from_tag_i, to_tag_i], imp_value\n",
    "#                             )\n",
    "    ### END MODIFIED SECTION: CRF OUTPUT ###\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db8ed822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag_is: \n",
      "{'B': [2, 4, 5, 9, 10, 11, 12, 13, 14, 15], 'I': [3, 6, 7, 8, 16, 17, 18, 19, 20], 'O': [1], 'P': []}\n"
     ]
    }
   ],
   "source": [
    "bilstm = BiLSTM(\n",
    "    input_dim=len(corpus.word_field.vocab),\n",
    "    embedding_dim=300,\n",
    "    char_emb_dim=25,\n",
    "    char_input_dim=len(corpus.char_field.vocab),\n",
    "    char_cnn_filter_num=5,\n",
    "    char_cnn_kernel_size=3,\n",
    "    hidden_dim=64,\n",
    "    output_dim=len(corpus.tag_field.vocab),\n",
    "    lstm_layers=2,\n",
    "    emb_dropout=0.5,\n",
    "    cnn_dropout=0.25,\n",
    "    lstm_dropout=0.1,\n",
    "    fc_dropout=0.25,\n",
    "    word_pad_idx=corpus.word_pad_idx,\n",
    "    char_pad_idx=corpus.char_pad_idx,\n",
    "    tag_pad_idx=corpus.tag_pad_idx\n",
    ")\n",
    "\n",
    "bilstm.init_embeddings(\n",
    "    char_pad_idx=corpus.char_pad_idx,\n",
    "    word_pad_idx=corpus.word_pad_idx\n",
    ")\n",
    "\n",
    "# CRF transitions initialization for impossible transitions\n",
    "bilstm.init_crf_transitions(\n",
    "    tag_names=corpus.tag_field.vocab.itos\n",
    ")\n",
    "# print(f\"The model has {bilstm.count_parameters():,} trainable parameters.\")\n",
    "# print(bilstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "459df029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "\n",
    "    def __init__(self, model, data, optimizer_cls, loss_fn_cls):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.optimizer = optimizer_cls(model.parameters())\n",
    "        self.loss_fn = loss_fn_cls(ignore_index=self.data.tag_pad_idx)\n",
    "\n",
    "    @staticmethod\n",
    "    def epoch_time(start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "        return elapsed_mins, elapsed_secs\n",
    "\n",
    "    def accuracy(self, preds, y):\n",
    "        max_preds = preds.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
    "        non_pad_elements = (y != self.data.tag_pad_idx).nonzero()  # prepare masking for paddings\n",
    "        correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "        return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])\n",
    "\n",
    "    def epoch(self):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        self.model.train()\n",
    "        for batch in self.data.train_iter:\n",
    "            # words = [sent len, batch size]\n",
    "            words = batch.word\n",
    "            # chars = [batch size, sent len, char len]\n",
    "            chars = batch.char  # NEWLY ADDED\n",
    "            # tags = [sent len, batch size]\n",
    "            true_tags = batch.tag\n",
    "            self.optimizer.zero_grad()\n",
    "            pred_tags = self.model(words, chars)  # MODIFIED\n",
    "            # to calculate the loss and accuracy, we flatten both prediction and true tags\n",
    "            # flatten pred_tags to [sent len, batch size, output dim]\n",
    "            pred_tags = pred_tags.view(-1, pred_tags.shape[-1])\n",
    "            # flatten true_tags to [sent len * batch size]\n",
    "            true_tags = true_tags.view(-1)\n",
    "            batch_loss = self.loss_fn(pred_tags, true_tags)\n",
    "            batch_acc = self.accuracy(pred_tags, true_tags)\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            epoch_loss += batch_loss.item()\n",
    "            epoch_acc += batch_acc.item()\n",
    "        return epoch_loss / len(self.data.train_iter), epoch_acc / len(self.data.train_iter)\n",
    "\n",
    "    def evaluate(self, iterator):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # similar to epoch() but model is in evaluation mode and no backprop\n",
    "            for batch in iterator:\n",
    "                words = batch.word\n",
    "                chars = batch.char  # NEWLY ADDED\n",
    "                true_tags = batch.tag\n",
    "                pred_tags = self.model(words, chars)  # MODIFIED\n",
    "                pred_tags = pred_tags.view(-1, pred_tags.shape[-1])\n",
    "                true_tags = true_tags.view(-1)\n",
    "                batch_loss = self.loss_fn(pred_tags, true_tags)\n",
    "                batch_acc = self.accuracy(pred_tags, true_tags)\n",
    "                epoch_loss += batch_loss.item()\n",
    "                epoch_acc += batch_acc.item()\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        for epoch in range(n_epochs):\n",
    "            start_time = time.time()\n",
    "            train_loss, train_acc = self.epoch()\n",
    "            end_time = time.time()\n",
    "            epoch_mins, epoch_secs = Trainer.epoch_time(start_time, end_time)\n",
    "            print(f\"Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "            print(f\"\\tTrn Loss: {train_loss:.3f} | Trn Acc: {train_acc * 100:.2f}%\")\n",
    "            val_loss, val_acc = self.evaluate(self.data.val_iter)\n",
    "            print(f\"\\tVal Loss: {val_loss:.3f} | Val Acc: {val_acc * 100:.2f}%\")\n",
    "        test_loss, test_acc = self.evaluate(self.data.test_iter)\n",
    "        print(f\"Test Loss: {test_loss:.3f} |  Test Acc: {test_acc * 100:.2f}%\")\n",
    "\n",
    "    def infer(self, sentence, true_tags=None):\n",
    "        self.model.eval()\n",
    "        # tokenize sentence\n",
    "        nlp = Indonesian()\n",
    "        tokens = [token.text for token in nlp(sentence)]\n",
    "        max_word_len = max([len(token) for token in tokens])\n",
    "        # transform to indices based on corpus vocab\n",
    "        numericalized_tokens = [self.data.word_field.vocab.stoi[token.lower()] for token in tokens]\n",
    "        ### BEGIN MODIFIED SECTION: CHARACTER EMBEDDING ###\n",
    "        numericalized_chars = []\n",
    "        char_pad_id = self.data.char_pad_idx\n",
    "        for token in tokens:\n",
    "            numericalized_chars.append(\n",
    "                [self.data.char_field.vocab.stoi[char] for char in token]\n",
    "                + [char_pad_id for _ in range(max_word_len - len(token))]\n",
    "            )\n",
    "        ### END MODIFIED SECTION ###\n",
    "        # find unknown words\n",
    "        unk_idx = self.data.word_field.vocab.stoi[self.data.word_field.unk_token]\n",
    "        unks = [t for t, n in zip(tokens, numericalized_tokens) if n == unk_idx]\n",
    "        # begin prediction\n",
    "        token_tensor = torch.as_tensor(numericalized_tokens)\n",
    "        token_tensor = token_tensor.unsqueeze(-1)\n",
    "        char_tensor = torch.as_tensor(numericalized_chars)  # NEWLY ADDED\n",
    "        char_tensor = char_tensor.unsqueeze(0)  # NEWLY ADDED: batch size at the beginning\n",
    "        predictions = self.model(token_tensor, char_tensor)  # MODIFIED\n",
    "        # convert results to tags\n",
    "        top_predictions = predictions.argmax(-1)\n",
    "        predicted_tags = [self.data.tag_field.vocab.itos[t.item()] for t in top_predictions]\n",
    "        # print inferred tags\n",
    "        max_len_token = max([len(token) for token in tokens] + [len('word')])\n",
    "        max_len_tag = max([len(tag) for tag in predicted_tags] + [len('pred')])\n",
    "        print(\n",
    "            f\"{'word'.ljust(max_len_token)}\\t{'unk'.ljust(max_len_token)}\\t{'pred tag'.ljust(max_len_tag)}\"\n",
    "            + (\"\\ttrue tag\" if true_tags else \"\")\n",
    "        )\n",
    "        for i, token in enumerate(tokens):\n",
    "            is_unk = \"✓\" if token in unks else \"\"\n",
    "            print(\n",
    "                f\"{token.ljust(max_len_token)}\\t{is_unk.ljust(max_len_token)}\\t{predicted_tags[i].ljust(max_len_tag)}\"\n",
    "                + (f\"\\t{true_tags[i]}\" if true_tags else \"\")\n",
    "            )\n",
    "        return tokens, predicted_tags, unks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "05d46a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 26s\n",
      "\tTrn Loss: 1.199 | Trn Acc: 76.50%\n",
      "\tVal Loss: 0.852 | Val Acc: 75.48%\n",
      "Epoch: 02 | Epoch Time: 1m 21s\n",
      "\tTrn Loss: 0.431 | Trn Acc: 87.97%\n",
      "\tVal Loss: 0.294 | Val Acc: 92.74%\n",
      "Epoch: 03 | Epoch Time: 1m 25s\n",
      "\tTrn Loss: 0.179 | Trn Acc: 95.61%\n",
      "\tVal Loss: 0.187 | Val Acc: 95.18%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mbilstm,\n\u001b[0;32m      3\u001b[0m     data\u001b[38;5;241m=\u001b[39mcorpus,\n\u001b[0;32m      4\u001b[0m     optimizer_cls\u001b[38;5;241m=\u001b[39mAdam,\n\u001b[0;32m      5\u001b[0m     loss_fn_cls\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, n_epochs)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m     69\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 70\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     72\u001b[0m     epoch_mins, epoch_secs \u001b[38;5;241m=\u001b[39m Trainer\u001b[38;5;241m.\u001b[39mepoch_time(start_time, end_time)\n",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36mTrainer.epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m pred_tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(words, chars)  \u001b[38;5;66;03m# MODIFIED\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# to calculate the loss and accuracy, we flatten both prediction and true tags\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# flatten pred_tags to [sent len, batch size, output dim]\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m pred_tags \u001b[38;5;241m=\u001b[39m \u001b[43mpred_tags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_tags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# flatten true_tags to [sent len * batch size]\u001b[39;00m\n\u001b[0;32m     39\u001b[0m true_tags \u001b[38;5;241m=\u001b[39m true_tags\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=bilstm,\n",
    "    data=corpus,\n",
    "    optimizer_cls=Adam,\n",
    "    loss_fn_cls=nn.CrossEntropyLoss\n",
    ")\n",
    "trainer.train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0a1de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
